You are assessing an AI-generated responses on a given task or input based on a set of criteria. Here is the data:\n[BEGIN DATA]\n***\n[Task]:
```result-to-evaluate
{{ REAL_RESULT }}
```

Only evaluation criteria that represents true user satisfaction:

```criteria
{{ ACCURACY_DEFINITION }}
```

Please consider the user's anticipated outcomes and experiences:
```user-expectations
{{ USER_EXPECTATION }}
```

User Environment Awareness:
```environment-awareness
{{ ENVIRONMENT_AWARENESS }}
```

You are presented with the following AI-generated text for evaluation:


Your evaluation process should critically adhere to the following guidelines:

1. **Independent Analysis:** Delve into text, assessing how well it aligns with user expectations from prospective of the criteria. It is vital to anchor your analysis in these criteria, exploring the depth, relevance, and precision of the response.

2. **Contextual Understanding:** Reflect on the content within the boundaries of the user's expectations. How well does each piece of text meet these anticipated standards? Scrutinize the content for factual correctness, relevance, consistency, and any other domain-specific standards detailed in the criteria.

3. **Critical Discrepancies:** Identify and elaborate on any critical points of deviation. These are aspects where the content potentially falters in meeting the user expectations or the criteria. Your insights should delve into the 'why' behind these discrepancies, providing a well-rounded perspective.

4. **Self-reflective Critique:** Conclude with a self-assessment of your analysis. Discuss any potential blind spots, subjective interpretations, or other obstacles you encountered during your evaluation. This introspection is crucial for transparency and continuous improvement in the evaluation process.

5. **Constructive Summary:** End your evaluation with a constructive overview, pinpointing the strengths and potential areas for improvement. This feedback should not only be a critique but a guide that can drive the qualitative enhancement of future AI-generated text, ensuring alignment with both the criteria and user expectations.

Throughout your evaluation, maintain a balance between critical assessment and objective reasoning, ensuring that your feedback is grounded in the criteria provided while being insightful and forward-looking. The ultimate goal is to uphold the integrity and reliability of AI-driven textual applications through meticulous and thoughtful evaluation.

This feedback should not only be a critique but a guide that can drive the qualitative enhancement of future AI-generated text, ensuring alignment with both accuracy and user expectations:

- 100-80: 'Excellent score range' - The text meets the highest standards of the criteria in content and tone.
- 79-60: 'Good score range' - While the text meets many standards well, it shows some areas needing improvement without severe missteps.
- 59-40: 'Fair score range' - The text reflects a moderate level of quality, with noticeable issues in the criteria that require attention.
- 39-20: 'Poor score range' - Major flaws are evident in the text, indicating a lack of adherence to necessary standards, potentially leading to misinformation or disconnection with the audience.
- 19-0: 'Unacceptable score range' - The text is critically flawed, offering little to no value, likely leading to confusion, misinformation, or offense.

[IMPORTANT] Critical errors necessitate stringent downgrading: any severe issue related to the criteria must result in a markedly lower score, reflecting the content's compromised integrity.

Do not provide exact score number but range